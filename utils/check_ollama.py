import requests
from langchain_ollama import ChatOllama

# æ˜¾å¼æŒ‡å®š Ollama æœåŠ¡åœ°å€ï¼ˆWindows å»ºè®®ç”¨ 127.0.0.1ï¼Œè€Œä¸æ˜¯ localhost/::1ï¼‰
OLLAMA_HOST = "http://127.0.0.1:11434"

def check_ollama_server():
    """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ"""
    try:
        resp = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        if resp.status_code == 200:
            print("âœ… Ollama æœåŠ¡å·²è¿æ¥æˆåŠŸ")
            print("å¯ç”¨æ¨¡å‹åˆ—è¡¨:", [m["name"] for m in resp.json().get("models", [])])
            return True
        else:
            print("âš ï¸ Ollama æœåŠ¡è¿”å›å¼‚å¸¸:", resp.status_code)
    except Exception as e:
        print("âŒ æ— æ³•è¿æ¥ Ollama æœåŠ¡:", e)
    return False


def main():
    if not check_ollama_server():
        print("è¯·ç¡®è®¤ Ollama å·²å¯åŠ¨ï¼Œå¹¶æ‰§è¡Œè¿‡: ollama pull qwen3")
        return

    # åˆå§‹åŒ– LangChain Ollama
    llm = ChatOllama(
        model="qwen3:latest",                # ç¡®ä¿ä½ å·²æ‹‰å–: ollama pull llama3
        base_url=OLLAMA_HOST,          # æ˜¾å¼æŒ‡å®šæœåŠ¡åœ°å€
        temperature=0.7
    )

    # æµ‹è¯•è°ƒç”¨
    response = llm.invoke("ç”¨ä¸‰å¥è¯ä»‹ç»ä¸€ä¸‹é‡å­è®¡ç®—ã€‚")
    print("\nğŸ¤– æ¨¡å‹è¾“å‡º:\n", response.content)


if __name__ == "__main__":
    main()
